{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b09175f-a278-4f16-bcc3-b8ffef8a7689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, LabelEncoder\n",
    "\n",
    "UNKNOWN = \"UNK\"\n",
    "PADDING = \"PAD\"\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, filename, split_indices, lower=True, max_len=None, vocab=None):\n",
    "        super().__init__()\n",
    "\n",
    "        texts, labels, numeric_data = self.read_csv(filename, split_indices)\n",
    "        \n",
    "        if vocab is None:\n",
    "            self.vocab = self.get_vocab(texts, lower)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.data_tensors, self.lengths_tensors = self.convert_text_to_tensors(texts, lower)\n",
    "        self.numeric_tensors = self.convert_numeric_to_tensors(numeric_data)    \n",
    "        self.labels_tensors = self.convert_labels_to_tensors(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {'x': self.data_tensors[idx],\n",
    "                'lengths': self.lengths_tensors[idx],\n",
    "                'numeric': self.numeric_tensors[idx],\n",
    "                'y': self.labels_tensors[idx],\n",
    "               }\n",
    "        return data\n",
    "\n",
    "    def get_vocab(self, texts, lower):\n",
    "        \n",
    "        vocab = {PADDING: 0, UNKNOWN: 1}\n",
    "        \n",
    "        for t in texts:\n",
    "            if lower:\n",
    "                texts = t.lower()\n",
    "            words = t.split()\n",
    "        \n",
    "            for word in words:\n",
    "                if word not in vocab:\n",
    "                    vocab [word] = len(vocab)\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "    def pad(self, idx_list):\n",
    "        if self.max_len is None:\n",
    "            self.max_len = 0\n",
    "            for instance in idx_list:\n",
    "                if len(instance) > self.max_len:\n",
    "                    self.max_len = len(instance)\n",
    "        padded_list = []\n",
    "        original_lengths = []\n",
    "        \n",
    "        for seq in idx_list:\n",
    "            original_lengths.append(len(seq))\n",
    "            if len(seq) > self.max_len:\n",
    "                seq = seq[:self.max_len]\n",
    "\n",
    "            else:\n",
    "                seq = seq + [self.vocab[PADDING]] * (self.max_len - len(seq))\n",
    "                padded_list.append(seq)\n",
    "\n",
    "        return padded_list, original_lengths\n",
    "\n",
    "    \n",
    "    \n",
    "    def read_csv(self, filename, split_indices, lower=True):\n",
    "        texts = []\n",
    "        numeric_features = []\n",
    "        labels = []\n",
    "        \n",
    "        with open(filename, encoding = \"utf-8\") as csvfile:\n",
    "            csvreader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "            for row in csvreader:\n",
    "                if not row['id'] in split_indices:\n",
    "                    continue\n",
    "                \n",
    "                # TODO: adapt to the inputs and output of your task\n",
    "                text = row['title']\n",
    "                if lower and text is not None:\n",
    "                    text = text.lower()\n",
    "                texts.append(text)\n",
    "\n",
    "                try:\n",
    "                    vote_average = float(row['vote_average'])\n",
    "                    runtime = float(row['runtime'])\n",
    "                    vote_count = float(row['vote_count'])\n",
    "                except ValueError:\n",
    "                # Falls ein Fehler auftritt (z.B. ungültige Werte), mit 0 füllen\n",
    "                    vote_average = runtime = vote_count = 0.0\n",
    "\n",
    "                numeric_features.append([runtime, vote_average, vote_count])\n",
    "                labels.append(float(row['revenue']))\n",
    "\n",
    "        numeric_features = np.array(numeric_features)\n",
    "\n",
    "        # Z-Score Normierung der Werte:\n",
    "        numeric_data = (numeric_features - numeric_features.mean(axis=0)) / numeric_features.std(axis=0)\n",
    "\n",
    "       \n",
    "        return texts, labels, numeric_data\n",
    "\n",
    "    def convert_text_to_tensors(self, text, lower):\n",
    "        python_list = []\n",
    "        for text_instance in text:\n",
    "            if lower:\n",
    "                text_instance = text_instance.lower()\n",
    "            idx_instance = []\n",
    "            for word in text_instance.split():\n",
    "                if not word in self.vocab:\n",
    "                    word = UNKNOWN\n",
    "                idx = self.vocab[word]\n",
    "                idx_instance.append(idx)\n",
    "            python_list.append(idx_instance)\n",
    "        # the instances in the list are of different length\n",
    "        # for numpy arrays and pytorch tensors we need the same length\n",
    "        # solution: padding and cropping\n",
    "        print(len(python_list))\n",
    "        python_list_padded, instance_lengths = self.pad(python_list)\n",
    "        print(len(python_list_padded))\n",
    "        vectors_numpy = np.array(python_list_padded)\n",
    "        tensors = torch.from_numpy(vectors_numpy)\n",
    "        lengths_tensors = torch.from_numpy(np.array(instance_lengths))\n",
    "        return tensors, lengths_tensors\n",
    "        \n",
    "    def convert_labels_to_tensors(self, labels):\n",
    "        label_tensors = torch.from_numpy(np.array(labels))\n",
    "        return label_tensors\n",
    "\n",
    "    def convert_numeric_to_tensors(self, numeric_data):\n",
    "        numeric_tensors = torch.from_numpy(np.array(numeric_data))\n",
    "        return numeric_tensors\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6423982-2933-4254-871a-053fe9acb40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9293\n",
      "9293\n",
      "1161\n",
      "1161\n",
      "1161\n",
      "1161\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv as csv\n",
    "\n",
    "random.seed(111)\n",
    "\n",
    "def get_indices_split(filename, split=[0.8, 0.1, 0.1]):\n",
    "    ids = []\n",
    "    with open(filename, encoding = \"utf-8\") as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in csvreader:\n",
    "            ids.append(row['id'])\n",
    "            # TODO: remove before doing the actual experiments!\n",
    "            #if len(ids) > 5000:\n",
    "           #     break\n",
    "    random.shuffle(ids)\n",
    "    start_idx = 0\n",
    "    splitted_ids = []\n",
    "    for part in split:\n",
    "        part_length = int(part * len(ids))\n",
    "        splitted_ids.append(ids[start_idx:start_idx+part_length])\n",
    "        start_idx += part_length\n",
    "    return splitted_ids\n",
    "\n",
    "filename = \"/Users/nadialitzenburger/Downloads/TMDB_movie_dataset_v11_cleaned.csv\"\n",
    "\n",
    "train_indices, dev_indices, test_indices = get_indices_split(filename)\n",
    "\n",
    "train_set = MovieDataset(filename=filename,\n",
    "                          split_indices=train_indices\n",
    "                          )\n",
    "dev_set = MovieDataset(filename=filename,\n",
    "                        split_indices=dev_indices,\n",
    "                        max_len=train_set.max_len,\n",
    "                        vocab=train_set.vocab,\n",
    "                       )\n",
    "test_set = MovieDataset(filename=filename,\n",
    "                         split_indices=test_indices,\n",
    "                         max_len=train_set.max_len,\n",
    "                         vocab=train_set.vocab,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab919fcb-949a-4d7a-9282-e150de397254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "torch.manual_seed(111)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Stellt sicher, dass alle Daten korrekt zu Tensoren zusammengefasst werden.\n",
    "    \"\"\"\n",
    "    x = torch.stack([item['x'] for item in batch])\n",
    "    lengths = torch.stack([item['lengths'] for item in batch])\n",
    "    numeric = torch.stack([item['numeric'] for item in batch])  # Numerische Features als Tensor\n",
    "    y = torch.stack([item['y'] for item in batch])\n",
    "\n",
    "    return {'x': x, 'lengths': lengths, 'numeric': numeric, 'y': y}\n",
    "\n",
    "# DataLoader mit collate_fn für korrektes Batch-Handling\n",
    "train_dataloader = DataLoader(train_set, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
    "dev_dataloader = DataLoader(dev_set, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac078d2e-5318-4f19-b4ce-40febc14b3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([32, 17])\n",
      "Shape of numeric_features: torch.Size([32, 3])\n",
      "Shape of y: torch.Size([32]) torch.float64\n",
      "Beispiel für X: tensor([[8864,  358,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [  62,   63, 8354,   11,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [  26,   27,   59,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   4, 4688,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 149,  108,    4,  176,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [  54,  287, 2145,  384,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [  21,   22,   23,    4,    1,  614,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [7396,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   8,   59,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   4,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [  62, 3757,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [  54, 6422,  729,  384,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [  54,  410,    4,  861,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   4,  118,    4,  437,   15,    4,  484,    1,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 653,  216, 4367,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [7331,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 223,  146,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 404,   40,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 103,  353,  354,   59,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   4,  257,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [4249,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 503,    1,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   4,  733,    1,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [2794,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   4, 6915,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 605,  606,  607,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 105,  973,    4, 2267,   15,    1,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [6565,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 184, 1413,  196,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 308,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   4,  326,   23,    4,  328,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [ 247,  108,  248,  249,  250,   59,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0]])\n",
      "Beispiel für numeric_features: tensor([[ 1.4146,  1.0631,  8.1093],\n",
      "        [ 1.6928,  0.7168,  7.8145],\n",
      "        [ 0.9820,  0.4722,  7.1373],\n",
      "        [ 2.5581,  1.2825,  6.7279],\n",
      "        [ 0.7347,  1.1124,  6.6387],\n",
      "        [ 1.7237,  0.9722,  6.5845],\n",
      "        [ 1.8782,  0.8453,  6.5192],\n",
      "        [ 0.7347,  0.8034,  5.9262],\n",
      "        [ 0.8583,  0.7566,  5.8351],\n",
      "        [ 0.2402,  0.6787,  5.6409],\n",
      "        [ 0.9820,  0.4757,  5.2026],\n",
      "        [ 1.1365,  0.7371,  5.1412],\n",
      "        [ 0.7656,  1.1514,  4.9625],\n",
      "        [ 1.6001,  0.6774,  4.6882],\n",
      "        [ 0.4875,  0.3822,  4.6305],\n",
      "        [ 0.0239,  0.5488,  4.4188],\n",
      "        [ 2.2182,  0.7791,  4.3201],\n",
      "        [-0.3470,  0.6943,  4.3069],\n",
      "        [ 0.9202,  0.6835,  4.1972],\n",
      "        [ 0.4875,  0.8233,  4.1467],\n",
      "        [ 0.4566,  0.2870,  4.0098],\n",
      "        [ 0.9202,  0.5999,  3.9392],\n",
      "        [ 0.5184,  0.7099,  3.8489],\n",
      "        [ 1.8782,  1.0159,  3.7195],\n",
      "        [ 0.6111,  0.7778,  3.6949],\n",
      "        [-0.0688,  0.6475,  3.6266],\n",
      "        [ 1.2910,  0.4900,  3.5127],\n",
      "        [ 0.9820,  0.8289,  3.2616],\n",
      "        [ 1.3219,  0.9120,  3.2365],\n",
      "        [ 1.0747,  0.5930,  3.2236],\n",
      "        [ 0.4257,  0.5285,  3.1348],\n",
      "        [ 0.3021,  0.8350,  3.0294]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    X = batch['x']  # Texte (Filmtitel als Indizes)\n",
    "    numeric_features = batch['numeric']  # Numerische Features (vote_average, runtime, vote_count)\n",
    "    y = batch['y']  # Umsatz (Label)\n",
    "\n",
    "    print(f\"Shape of X: {X.shape}\")  # Größe der Titel-Daten\n",
    "    print(f\"Shape of numeric_features: {numeric_features.shape}\")  # Größe der numerischen Features\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")  # Größe der Labels (Zielwerte)\n",
    "\n",
    "    print(\"Beispiel für X:\", X)\n",
    "    print(\"Beispiel für numeric_features:\", numeric_features)\n",
    "    \n",
    "    break  # Nur einen Batch ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecc6c983-4c36-4631-9303-b44b7fd69480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a363549-f1bb-4250-a5be-a40e6043fdc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieMLP(\n",
      "  (embeddings): Embedding(9286, 100, padding_idx=9285)\n",
      "  (numeric_layer): Linear(in_features=3, out_features=82, bias=True)\n",
      "  (linear1): Linear(in_features=182, out_features=118, bias=True)\n",
      "  (linear2): Linear(in_features=118, out_features=110, bias=True)\n",
      "  (linear3): Linear(in_features=110, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "config = {'num_classes': 1,\n",
    "          'embedding_dim': 100, #100-300\n",
    "          'hidden_dim1': 118,\n",
    "          'hidden_dim2': 110,\n",
    "          'hidden_dim_numeric': 82,\n",
    "          'vocab_size': len(train_set.vocab),\n",
    "         }\n",
    "\n",
    "# PART 2: Define model class\n",
    "\n",
    "# PART 2: Define the neural network\n",
    "\n",
    "class MovieMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Word Embeddings\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=config['vocab_size'],\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            padding_idx=config['vocab_size'] - 1\n",
    "        )\n",
    "\n",
    "        # Numerische Features werden durch eine eigene Schicht verarbeitet\n",
    "        self.numeric_layer = nn.Linear(in_features=3, out_features=config['hidden_dim_numeric'])\n",
    "        \n",
    "        # Korrigierte Input-Größe\n",
    "        combined_input_dim = config['embedding_dim'] + config['hidden_dim_numeric']\n",
    "\n",
    "        # MLP-Schichten\n",
    "        self.linear1 = nn.Linear(in_features=combined_input_dim, out_features=config['hidden_dim1'])\n",
    "        self.linear2 = nn.Linear(in_features=config['hidden_dim1'], out_features=config['hidden_dim2'])\n",
    "        self.linear3 = nn.Linear(in_features=config['hidden_dim2'], out_features=config['num_classes'])\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, lengths, numeric_features):\n",
    "        # Word Embeddings\n",
    "        emb = self.embeddings(x)\n",
    "        sentence = emb.sum(dim=1) / lengths.view(-1, 1)\n",
    "\n",
    "        # Numerische Features verarbeiten\n",
    "        numeric_out = self.relu(self.numeric_layer(numeric_features))\n",
    "\n",
    "        # Text + numerische Features kombinieren\n",
    "        combined = torch.cat((sentence, numeric_out), dim=1)  # Jetzt richtige Größe\n",
    "\n",
    "        # Durch das MLP-Netzwerk\n",
    "        z1 = self.relu(self.linear1(combined))\n",
    "        z2 = self.relu(self.linear2(z1))\n",
    "        logits = self.linear3(z2)  # KEIN ReLU nach der letzten Schicht!\n",
    "\n",
    "        return logits\n",
    "\n",
    "model = MovieMLP(config).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8203f9c3-f3fa-4fb2-acee-128a771e1dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieMLP(\n",
      "  (embeddings): Embedding(9286, 100, padding_idx=9285)\n",
      "  (numeric_layer): Linear(in_features=3, out_features=82, bias=True)\n",
      "  (linear1): Linear(in_features=182, out_features=118, bias=True)\n",
      "  (linear2): Linear(in_features=118, out_features=110, bias=True)\n",
      "  (linear3): Linear(in_features=110, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "<generator object Module.parameters at 0x17bc311c0>\n",
      "\n",
      "Epoch 1/25\n",
      "loss: 5783622288146432.000000  [   32/ 9293]\n",
      "loss: 21328212738965504.000000  [ 3232/ 9293]\n",
      "loss: 501677882867712.000000  [ 6432/ 9293]\n",
      "Avg loss: 10157749888827948.000000 \n",
      "\n",
      "R2 Score: 0.556463\n",
      "New best model with R2 score: 0.5564633013443969\n",
      "\n",
      "Epoch 2/25\n",
      "loss: 7300390525599744.000000  [   32/ 9293]\n",
      "loss: 5377717813903360.000000  [ 3232/ 9293]\n",
      "loss: 196606188060672.000000  [ 6432/ 9293]\n",
      "Avg loss: 9321399750526256.000000 \n",
      "\n",
      "R2 Score: 0.593119\n",
      "New best model with R2 score: 0.5931187096147854\n",
      "\n",
      "Epoch 3/25\n",
      "loss: 2857443889512448.000000  [   32/ 9293]\n",
      "loss: 285104895164416.000000  [ 3232/ 9293]\n",
      "loss: 2850412491177984.000000  [ 6432/ 9293]\n",
      "Avg loss: 9207597895500870.000000 \n",
      "\n",
      "R2 Score: 0.597987\n",
      "New best model with R2 score: 0.5979871992218366\n",
      "\n",
      "Epoch 4/25\n",
      "loss: 3695693998850048.000000  [   32/ 9293]\n",
      "loss: 371024843505664.000000  [ 3232/ 9293]\n",
      "loss: 1341536793001984.000000  [ 6432/ 9293]\n",
      "Avg loss: 9279037475369956.000000 \n",
      "\n",
      "R2 Score: 0.594907\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 5/25\n",
      "loss: 43785182511104.000000  [   32/ 9293]\n",
      "loss: 20350216473411584.000000  [ 3232/ 9293]\n",
      "loss: 11252250601062400.000000  [ 6432/ 9293]\n",
      "Avg loss: 9032491213181702.000000 \n",
      "\n",
      "R2 Score: 0.605648\n",
      "New best model with R2 score: 0.605647863048827\n",
      "\n",
      "Epoch 6/25\n",
      "loss: 2019746086125568.000000  [   32/ 9293]\n",
      "loss: 49760443119435776.000000  [ 3232/ 9293]\n",
      "loss: 27868440760942592.000000  [ 6432/ 9293]\n",
      "Avg loss: 9285175171984634.000000 \n",
      "\n",
      "R2 Score: 0.594684\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 7/25\n",
      "loss: 4015337544613888.000000  [   32/ 9293]\n",
      "loss: 4038136573198336.000000  [ 3232/ 9293]\n",
      "loss: 4557590587506688.000000  [ 6432/ 9293]\n",
      "Avg loss: 9035774282067636.000000 \n",
      "\n",
      "R2 Score: 0.605529\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 8/25\n",
      "loss: 2209783222370304.000000  [   32/ 9293]\n",
      "loss: 7996899466412032.000000  [ 3232/ 9293]\n",
      "loss: 266191822127104.000000  [ 6432/ 9293]\n",
      "Avg loss: 9016909195925116.000000 \n",
      "\n",
      "R2 Score: 0.606420\n",
      "New best model with R2 score: 0.6064195288260581\n",
      "\n",
      "Epoch 9/25\n",
      "loss: 3732120421793792.000000  [   32/ 9293]\n",
      "loss: 8319062278930432.000000  [ 3232/ 9293]\n",
      "loss: 1289979602927616.000000  [ 6432/ 9293]\n",
      "Avg loss: 9366244585394646.000000 \n",
      "\n",
      "R2 Score: 0.591091\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 10/25\n",
      "loss: 18659124640219136.000000  [   32/ 9293]\n",
      "loss: 1372655307456512.000000  [ 3232/ 9293]\n",
      "loss: 371734351970304.000000  [ 6432/ 9293]\n",
      "Avg loss: 9434254533837242.000000 \n",
      "\n",
      "R2 Score: 0.588114\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 11/25\n",
      "loss: 3184894880514048.000000  [   32/ 9293]\n",
      "loss: 639714776317952.000000  [ 3232/ 9293]\n",
      "loss: 3481102937227264.000000  [ 6432/ 9293]\n",
      "Avg loss: 9371455550923416.000000 \n",
      "\n",
      "R2 Score: 0.590873\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 12/25\n",
      "loss: 2039012504109056.000000  [   32/ 9293]\n",
      "loss: 1099589406949376.000000  [ 3232/ 9293]\n",
      "loss: 2320186430455808.000000  [ 6432/ 9293]\n",
      "Avg loss: 9256103632821608.000000 \n",
      "\n",
      "R2 Score: 0.595892\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 13/25\n",
      "loss: 519850191486976.000000  [   32/ 9293]\n",
      "loss: 967551240634368.000000  [ 3232/ 9293]\n",
      "loss: 435741041623040.000000  [ 6432/ 9293]\n",
      "Avg loss: 9108871600158554.000000 \n",
      "\n",
      "R2 Score: 0.602312\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 14/25\n",
      "loss: 1889388594200576.000000  [   32/ 9293]\n",
      "loss: 3834516468662272.000000  [ 3232/ 9293]\n",
      "loss: 493962880090112.000000  [ 6432/ 9293]\n",
      "Avg loss: 9065004568444042.000000 \n",
      "\n",
      "R2 Score: 0.604235\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 15/25\n",
      "loss: 309066014392320.000000  [   32/ 9293]\n",
      "loss: 50350734834663424.000000  [ 3232/ 9293]\n",
      "loss: 328136810037248.000000  [ 6432/ 9293]\n",
      "Avg loss: 8952621283305417.000000 \n",
      "\n",
      "R2 Score: 0.609193\n",
      "New best model with R2 score: 0.6091925968782933\n",
      "\n",
      "Epoch 16/25\n",
      "loss: 1096787410550784.000000  [   32/ 9293]\n",
      "loss: 1621420886982656.000000  [ 3232/ 9293]\n",
      "loss: 918433793310720.000000  [ 6432/ 9293]\n",
      "Avg loss: 9281071550619870.000000 \n",
      "\n",
      "R2 Score: 0.594811\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 17/25\n",
      "loss: 5635257474744320.000000  [   32/ 9293]\n",
      "loss: 2374529309474816.000000  [ 3232/ 9293]\n",
      "loss: 3201348363354112.000000  [ 6432/ 9293]\n",
      "Avg loss: 8996413115331114.000000 \n",
      "\n",
      "R2 Score: 0.607282\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 18/25\n",
      "loss: 53198259497205760.000000  [   32/ 9293]\n",
      "loss: 3225892222402560.000000  [ 3232/ 9293]\n",
      "loss: 624183973249024.000000  [ 6432/ 9293]\n",
      "Avg loss: 8967737130364840.000000 \n",
      "\n",
      "R2 Score: 0.608465\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 19/25\n",
      "loss: 1974241310277632.000000  [   32/ 9293]\n",
      "loss: 2189151675875328.000000  [ 3232/ 9293]\n",
      "loss: 11485220364615680.000000  [ 6432/ 9293]\n",
      "Avg loss: 9095180989638380.000000 \n",
      "\n",
      "R2 Score: 0.602927\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 20/25\n",
      "loss: 693930685366272.000000  [   32/ 9293]\n",
      "loss: 168920929533952.000000  [ 3232/ 9293]\n",
      "loss: 1673161552691200.000000  [ 6432/ 9293]\n",
      "Avg loss: 9063371505611578.000000 \n",
      "\n",
      "R2 Score: 0.604318\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 21/25\n",
      "loss: 910537831481344.000000  [   32/ 9293]\n",
      "loss: 10506316787220480.000000  [ 3232/ 9293]\n",
      "loss: 6872003508174848.000000  [ 6432/ 9293]\n",
      "Avg loss: 8800438081780874.000000 \n",
      "\n",
      "R2 Score: 0.615831\n",
      "New best model with R2 score: 0.6158311809374357\n",
      "\n",
      "Epoch 22/25\n",
      "loss: 2081005338886144.000000  [   32/ 9293]\n",
      "loss: 4649338068271104.000000  [ 3232/ 9293]\n",
      "loss: 2506737261215744.000000  [ 6432/ 9293]\n",
      "Avg loss: 8863226064457617.000000 \n",
      "\n",
      "R2 Score: 0.613157\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 23/25\n",
      "loss: 19083634912788480.000000  [   32/ 9293]\n",
      "loss: 14543211309563904.000000  [ 3232/ 9293]\n",
      "loss: 260534662332416.000000  [ 6432/ 9293]\n",
      "Avg loss: 9627446846162446.000000 \n",
      "\n",
      "R2 Score: 0.579625\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 24/25\n",
      "loss: 1300894859657216.000000  [   32/ 9293]\n",
      "loss: 1253107174473728.000000  [ 3232/ 9293]\n",
      "loss: 6465871400665088.000000  [ 6432/ 9293]\n",
      "Avg loss: 8964233135467918.000000 \n",
      "\n",
      "R2 Score: 0.608606\n",
      "No improvement in model learning rate 0.020011\n",
      "\n",
      "Epoch 25/25\n",
      "loss: 4309148204269568.000000  [   32/ 9293]\n",
      "loss: 6367549398712320.000000  [ 3232/ 9293]\n",
      "loss: 396922019905536.000000  [ 6432/ 9293]\n",
      "Avg loss: 8959891151179665.000000 \n",
      "\n",
      "R2 Score: 0.608809\n",
      "No improvement in model learning rate 0.020011\n"
     ]
    }
   ],
   "source": [
    "#from sklearn.metrics import f1_score, classification_report, mean_squared_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = MovieMLP(config).to(device)\n",
    "print(model)\n",
    "print(model.parameters())\n",
    "\n",
    "# PART 3: Training loop\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "# PART 3b: Training function\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()  # model will be trained here (important for some layers)\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X = batch['x'].to(device)\n",
    "        lengths = batch['lengths'].to(device)\n",
    "        numeric_features = batch['numeric'].to(device).float()  # Numerische Features hinzufügen!\n",
    "        y = batch['y'].to(device).float()\n",
    "\n",
    "        # Compute prediction and loss function\n",
    "        pred = model(X, lengths, numeric_features)[:, 0]  # Numerische Features übergeben!\n",
    "     \n",
    "        # get predictions (forward pass)\n",
    "        loss = loss_fn(pred, y.float())  # calculate loss function\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()  # calculate gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5) #args.clip)\n",
    "        optimizer.step()  # update parameters\n",
    "        optimizer.zero_grad()  # set gradients back to zero\n",
    "\n",
    "        if batch_idx % 100 == 0:  # optional: print some statistics\n",
    "            loss, current = loss.item(), (batch_idx + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "# PART 3c: Testing function\n",
    "def test(dataloader, model, loss_fn):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()  # model will be evaluated here (important for some layers)\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():  # tell pytorch to do no updates here\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            X = batch['x'].to(device)\n",
    "            lengths = batch['lengths'].to(device)\n",
    "            numeric_features = batch['numeric'].to(device).float()  # Numerische Features hinzufügen!\n",
    "            y = batch['y'].to(device).float()\n",
    "\n",
    "            outputs = model(X, lengths, numeric_features)[:, 0]  # get predictions\n",
    "            test_loss += loss_fn(outputs, y).item()  # accumulate loss\n",
    "            # important: use item() in the line above or tonumpy() or something like this\n",
    "            # to accumulate only the value of the tensor without gradients / computation graph information\n",
    "            # otherwise: memory issues\n",
    "\n",
    "            predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            labels.extend(y.cpu().numpy().flatten())\n",
    "            \n",
    "    mse_score = r2_score(labels, predictions)\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    print(f\"Avg loss: {test_loss:>8f} \\n\")\n",
    "    print(f\"R2 Score: {mse_score:>8f}\")\n",
    "\n",
    "    #plt.hist(predictions)\n",
    "    #plt.show() \n",
    "    #plt.hist(labels)\n",
    "    #plt.show() \n",
    "    \n",
    "    return mse_score\n",
    "\n",
    "# PART 3d: Start the training\n",
    "epochs = 25\n",
    "\n",
    "#lr=0.0100111\n",
    "lr = 0.020011\n",
    "\n",
    "model = MovieMLP(config).to(device)\n",
    "\n",
    "# Initialize the optimizer with the current learning rate and weight decay = 0.0001001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr, weight_decay = 0.0001001)\n",
    "\n",
    "# Track best MSE score for early stopping or model saving\n",
    "best_mse_score = -1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)  # Train the model with the current optimizer\n",
    "\n",
    "    mse_score = test(dev_dataloader, model, loss_fn)  # Test the model with the current optimizer\n",
    "        \n",
    "    if mse_score > best_mse_score:  # Early stopping based on the best score\n",
    "        save_path = f\"model_lr_{lr:.4f}.pth\"  # Save model for each learning rate\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        best_mse_score = mse_score\n",
    "        print(f\"New best model with R2 score: {mse_score}\")\n",
    "    else:\n",
    "        print(f\"No improvement in model learning rate {lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fe4307f-299c-40eb-8090-3215bf28d8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing final model on test...\n",
      "Avg loss: 6323883946371404.000000 \n",
      "\n",
      "R2 Score: 0.646636\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zt/s9ljmn_s4rb2yxy_g0lbzdpc0000gn/T/ipykernel_53627/3565572165.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_path))\n"
     ]
    }
   ],
   "source": [
    "# PART 3e: Load and test the best model\n",
    "\n",
    "# if you want to load the best model from training before testing:\n",
    "model = MovieMLP(config).to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "print(\"testing final model on test...\")\n",
    "test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f78b2b-2106-492f-80c7-515a2c1ba625",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
