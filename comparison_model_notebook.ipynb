{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7b366952-4217-419f-97c2-1aa159f48346",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "UNKNOWN = \"UNK\"\n",
    "PADDING = \"PAD\"\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, filename, split_indices, lower=True, max_len=None, vocab=None):\n",
    "        super().__init__()\n",
    "\n",
    "        texts, labels, numeric_data = self.read_csv(filename, split_indices)\n",
    "        \n",
    "        if vocab is None:\n",
    "            self.vocab = self.get_vocab(texts, lower)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.data_tensors, self.lengths_tensors = self.convert_text_to_tensors(texts, lower)\n",
    "        self.numeric_tensors = self.convert_numeric_to_tensors(numeric_data)    \n",
    "        self.labels_tensors = self.convert_labels_to_tensors(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = {'x': self.data_tensors[idx],\n",
    "                'lengths': self.lengths_tensors[idx],\n",
    "                'numeric': self.numeric_tensors[idx],\n",
    "                'y': self.labels_tensors[idx],\n",
    "               }\n",
    "        return data\n",
    "\n",
    "    def get_vocab(self, texts, lower):\n",
    "        vocab = {PADDING: 0, UNKNOWN: 1}\n",
    "        \n",
    "        for t in texts:\n",
    "            if lower:\n",
    "                t = t.lower()\n",
    "            words = t.split()\n",
    "        \n",
    "            for word in words:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "        \n",
    "        return vocab\n",
    "\n",
    "    def pad(self, idx_list):\n",
    "        if self.max_len is None:\n",
    "            self.max_len = 0\n",
    "            for instance in idx_list:\n",
    "                if len(instance) > self.max_len:\n",
    "                    self.max_len = len(instance)\n",
    "        padded_list = []\n",
    "        original_lengths = []\n",
    "        \n",
    "        for seq in idx_list:\n",
    "            original_lengths.append(len(seq))\n",
    "            if len(seq) > self.max_len:\n",
    "                seq = seq[:self.max_len]\n",
    "            else:\n",
    "                seq = seq + [self.vocab[PADDING]] * (self.max_len - len(seq))\n",
    "            padded_list.append(seq)\n",
    "\n",
    "        return padded_list, original_lengths\n",
    "\n",
    "    def read_csv(self, filename, split_indices, lower=True):\n",
    "        texts = []\n",
    "        numeric_features = []\n",
    "        labels = []\n",
    "    \n",
    "        # First pass to collect valid values for median calculation\n",
    "        valid_months = []\n",
    "        valid_runtimes = []\n",
    "    \n",
    "        with open(filename, encoding=\"utf-8\") as csvfile:\n",
    "            csvreader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "            for row in csvreader:\n",
    "                if row['id'] not in split_indices:\n",
    "                    continue\n",
    "                \n",
    "                # Collect valid values\n",
    "                try:\n",
    "                    if row['release_date']:\n",
    "                        month = int(row['release_date'].split(\"-\")[1])\n",
    "                        valid_months.append(month)\n",
    "                    if row['runtime']:\n",
    "                        runtime = int(row['runtime'])\n",
    "                        valid_runtimes.append(runtime)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "    \n",
    "        # Calculate medians\n",
    "        median_month = int(np.median(valid_months))\n",
    "        median_runtime = int(np.median(valid_runtimes))\n",
    "    \n",
    "        # Second pass with a new file handle\n",
    "        with open(filename, encoding=\"utf-8\") as csvfile:\n",
    "            csvreader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "            for row in csvreader:\n",
    "                if row['id'] not in split_indices:\n",
    "                    continue\n",
    "            \n",
    "                text = row['title']\n",
    "                if lower and text is not None:\n",
    "                    text = text.lower()\n",
    "                texts.append(text)\n",
    "\n",
    "                try:\n",
    "                    release_date = row['release_date']\n",
    "                    release_month = int(release_date.split(\"-\")[1]) if release_date else median_month\n",
    "                    runtime = int(row['runtime']) if row['runtime'] else median_runtime\n",
    "                except ValueError:\n",
    "                    release_month = median_month\n",
    "                    runtime = median_runtime\n",
    "\n",
    "                numeric_features.append([release_month, runtime])\n",
    "                labels.append(float(row['revenue']))\n",
    "\n",
    "        numeric_features = np.array(numeric_features)\n",
    "    \n",
    "        # Z-Score normalization\n",
    "        means = numeric_features.mean(axis=0)\n",
    "        stds = numeric_features.std(axis=0)\n",
    "        stds[stds == 0] = 1  # Prevent division by zero\n",
    "        numeric_data = (numeric_features - means) / stds\n",
    "\n",
    "        return texts, labels, numeric_data\n",
    "\n",
    "    def convert_text_to_tensors(self, text, lower):\n",
    "        python_list = []\n",
    "        for text_instance in text:\n",
    "            if lower:\n",
    "                text_instance = text_instance.lower()\n",
    "            idx_instance = []\n",
    "            for word in text_instance.split():\n",
    "                if word not in self.vocab:\n",
    "                    word = UNKNOWN\n",
    "                idx = self.vocab[word]\n",
    "                idx_instance.append(idx)\n",
    "            python_list.append(idx_instance)\n",
    "        \n",
    "        python_list_padded, instance_lengths = self.pad(python_list)\n",
    "        vectors_numpy = np.array(python_list_padded)\n",
    "        tensors = torch.from_numpy(vectors_numpy)\n",
    "        lengths_tensors = torch.from_numpy(np.array(instance_lengths))\n",
    "        return tensors, lengths_tensors\n",
    "        \n",
    "    def convert_labels_to_tensors(self, labels):\n",
    "        label_tensors = torch.from_numpy(np.array(labels))\n",
    "        return label_tensors\n",
    "\n",
    "    def convert_numeric_to_tensors(self, numeric_data):\n",
    "        numeric_tensors = torch.from_numpy(np.array(numeric_data))\n",
    "        return numeric_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7a43d0dc-7b6d-4b87-aecf-b9ff6659f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv as csv\n",
    "\n",
    "random.seed(111)\n",
    "\n",
    "def get_indices_split(filename, split=[0.8, 0.1, 0.1]):\n",
    "    ids = []\n",
    "    with open(filename, encoding = \"utf-8\") as csvfile:\n",
    "        csvreader = csv.DictReader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in csvreader:\n",
    "            ids.append(row['id'])\n",
    "            # TODO: remove before doing the actual experiments!\n",
    "            if len(ids) > 5000:\n",
    "                break\n",
    "    random.shuffle(ids)\n",
    "    start_idx = 0\n",
    "    splitted_ids = []\n",
    "    for part in split:\n",
    "        part_length = int(part * len(ids))\n",
    "        splitted_ids.append(ids[start_idx:start_idx+part_length])\n",
    "        start_idx += part_length\n",
    "    return splitted_ids\n",
    "\n",
    "filename = \"/Users/nadialitzenburger/Downloads/TMDB_movie_dataset_v11_cleaned.csv\"\n",
    "\n",
    "train_indices, dev_indices, test_indices = get_indices_split(filename)\n",
    "\n",
    "train_set = MovieDataset(filename=filename,\n",
    "                          split_indices=train_indices\n",
    "                          )\n",
    "dev_set = MovieDataset(filename=filename,\n",
    "                        split_indices=dev_indices,\n",
    "                        max_len=train_set.max_len,\n",
    "                        vocab=train_set.vocab,\n",
    "                       )\n",
    "test_set = MovieDataset(filename=filename,\n",
    "                         split_indices=test_indices,\n",
    "                         max_len=train_set.max_len,\n",
    "                         vocab=train_set.vocab,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2833b3ff-5296-4b54-8fa9-e12fcdc44f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "torch.manual_seed(111)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Stellt sicher, dass alle Daten korrekt zu Tensoren zusammengefasst werden.\n",
    "    \"\"\"\n",
    "    x = torch.stack([item['x'] for item in batch])\n",
    "    lengths = torch.stack([item['lengths'] for item in batch])\n",
    "    numeric = torch.stack([item['numeric'] for item in batch])  # Numerische Features als Tensor\n",
    "    y = torch.stack([item['y'] for item in batch])\n",
    "\n",
    "    return {'x': x, 'lengths': lengths, 'numeric': numeric, 'y': y}\n",
    "\n",
    "# DataLoader mit collate_fn f√ºr korrektes Batch-Handling\n",
    "train_dataloader = DataLoader(train_set, shuffle=True, batch_size=batch_size, collate_fn=collate_fn)\n",
    "dev_dataloader = DataLoader(dev_set, batch_size=batch_size, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_set, batch_size=batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "74848a41-b005-46dd-bae2-e4a654597785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([32, 12])\n",
      "Shape of numeric_features: torch.Size([32, 2])\n",
      "Shape of y: torch.Size([32]) torch.float64\n",
      "Beispiel f√ºr X: tensor([[   1,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   4,   28,   16,    4,   29,    4,  152,   16,    4,  720,    0,    0],\n",
      "        [  20,   21,   50,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   4,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  71,  169,    4, 2002,    1,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 443,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   4,  635,  720,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 581,    4,    5,  164,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   4,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1394, 1394, 1342,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [3212,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  63,   16,    4,   64,  263,    1, 4169,    0,    0,    0,    0,    0],\n",
      "        [1104, 3520,  662,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,  584,   16,    4,    1,  365,    0,    0,    0,    0,    0,    0],\n",
      "        [ 331,  715,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 110,  111,   50,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 955,   16, 1196,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 519,   34,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [  89,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [1245, 2147,  168,    1,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 605,  606,  607,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [   4,  635,  720,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 519,  520,    4,    1,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 158,    4,  170, 1612,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 678,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0],\n",
      "        [ 880,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])\n",
      "Beispiel f√ºr numeric_features: tensor([[-0.2722,  1.5510],\n",
      "        [ 1.5265,  4.0497],\n",
      "        [-0.8718,  0.7887],\n",
      "        [ 0.6271,  1.5087],\n",
      "        [ 1.5265,  1.2969],\n",
      "        [ 0.6271,  1.2546],\n",
      "        [-0.2722, -0.6935],\n",
      "        [ 0.9269,  0.2805],\n",
      "        [-0.2722, -0.2277],\n",
      "        [ 1.2267,  1.0005],\n",
      "        [ 0.0276, -0.6935],\n",
      "        [ 0.0276,  1.9322],\n",
      "        [ 1.5265, -0.1853],\n",
      "        [-1.1715,  0.1111],\n",
      "        [ 1.5265,  1.0005],\n",
      "        [ 0.9269, -0.5665],\n",
      "        [-0.5720,  0.3652],\n",
      "        [-1.1715,  0.4923],\n",
      "        [ 0.9269,  1.4240],\n",
      "        [-0.5720, -0.3547],\n",
      "        [ 1.5265,  0.4499],\n",
      "        [-1.1715, -1.0323],\n",
      "        [-0.2722, -0.3547],\n",
      "        [ 1.5265,  0.5770],\n",
      "        [-0.5720,  0.7887],\n",
      "        [ 0.9269,  1.2122],\n",
      "        [-0.2722, -0.6512],\n",
      "        [ 0.0276,  0.5346],\n",
      "        [-1.1715, -0.6088],\n",
      "        [-0.5720, -0.0583],\n",
      "        [ 1.5265,  0.9158],\n",
      "        [ 0.9269,  0.5346]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for batch in test_dataloader:\n",
    "    X = batch['x']  # Texte (Filmtitel als Indizes)\n",
    "    numeric_features = batch['numeric']  # Numerische Features \n",
    "    y = batch['y']  # Umsatz (Label)\n",
    "\n",
    "    print(f\"Shape of X: {X.shape}\")  # Gr√∂√üe der Titel-Daten\n",
    "    print(f\"Shape of numeric_features: {numeric_features.shape}\")  # Gr√∂√üe der numerischen Features\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")  # Gr√∂√üe der Labels (Zielwerte)\n",
    "\n",
    "    print(\"Beispiel f√ºr X:\", X)\n",
    "    print(\"Beispiel f√ºr numeric_features:\", numeric_features)\n",
    "    \n",
    "    break  # Nur einen Batch ausgeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ba528e4e-2d62-432f-9e09-c9c91b1eea2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    \n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c8894f51-a78f-423e-b742-71818a575cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieMLP(\n",
      "  (embeddings): Embedding(4454, 128, padding_idx=0)\n",
      "  (numeric_layer): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (linear1): Linear(in_features=256, out_features=322, bias=True)\n",
      "  (linear2): Linear(in_features=322, out_features=290, bias=True)\n",
      "  (linear3): Linear(in_features=290, out_features=132, bias=True)\n",
      "  (linear4): Linear(in_features=132, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "config = {\n",
    "    'num_classes': 1,  \n",
    "    'embedding_dim': 128,  \n",
    "    'hidden_dim1': 322,\n",
    "    'hidden_dim2': 290,\n",
    "    'hidden_dim3': 132,  \n",
    "    'hidden_dim_numeric': 128,\n",
    "    'vocab_size': len(train_set.vocab),\n",
    "}\n",
    "\n",
    "class MovieMLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Word Embeddings\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=config['vocab_size'],\n",
    "            embedding_dim=config['embedding_dim'],\n",
    "            padding_idx=0  # Use 0 since PADDING is mapped to 0 in vocab\n",
    "        )\n",
    "        \n",
    "        # Numeric features layer - CHANGED from 3 to 2 features (month, runtime)\n",
    "        self.numeric_layer = nn.Linear(in_features=2, out_features=config['hidden_dim_numeric'])\n",
    "        \n",
    "        # Combined input dimension\n",
    "        combined_input_dim = config['embedding_dim'] + config['hidden_dim_numeric']\n",
    "        \n",
    "        # MLP layers - now with four linear layers instead of three\n",
    "        self.linear1 = nn.Linear(in_features=combined_input_dim, out_features=config['hidden_dim1'])\n",
    "        self.linear2 = nn.Linear(in_features=config['hidden_dim1'], out_features=config['hidden_dim2'])\n",
    "        self.linear3 = nn.Linear(in_features=config['hidden_dim2'], out_features=config['hidden_dim3'])\n",
    "        self.linear4 = nn.Linear(in_features=config['hidden_dim3'], out_features=config['num_classes'])\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, lengths, numeric_features):\n",
    "        # Word Embeddings\n",
    "        emb = self.embeddings(x)\n",
    "        # Average embeddings (accounting for padding)\n",
    "        sentence = emb.sum(dim=1) / lengths.view(-1, 1)\n",
    "        \n",
    "        # Process numeric features (2 features: month, runtime)\n",
    "        numeric_out = self.relu(self.numeric_layer(numeric_features))\n",
    "        \n",
    "        combined = torch.cat((sentence, numeric_out), dim=1)\n",
    "        \n",
    "        # MLP layers - now using all four layers\n",
    "        z1 = self.relu(self.linear1(combined))\n",
    "        z2 = self.relu(self.linear2(z1))\n",
    "        z3 = self.relu(self.linear3(z2))  # Added third hidden layer\n",
    "        logits = self.linear4(z3)  # Changed to linear4 for final output\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MovieMLP(config).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0f91ee1c-c9b5-4595-99f5-e9211482bf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MovieMLP(\n",
      "  (embeddings): Embedding(4454, 128, padding_idx=0)\n",
      "  (numeric_layer): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (linear1): Linear(in_features=256, out_features=322, bias=True)\n",
      "  (linear2): Linear(in_features=322, out_features=290, bias=True)\n",
      "  (linear3): Linear(in_features=290, out_features=132, bias=True)\n",
      "  (linear4): Linear(in_features=132, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      "Epoch 1/80\n",
      "loss: 25236630830841856.000000  [   32/ 4000]\n",
      "loss: 39262769953898496.000000  [ 3232/ 4000]\n",
      "Avg loss: 37314108227321856.000000\n",
      "R2 Score: 0.199247\n",
      "Training Loss: 37924017562424704.0000\n",
      "Validation Loss: 37314108227321856.0000\n",
      "Validation R¬≤ Score: 0.1992\n",
      "New best R¬≤ score: 0.1992\n",
      "\n",
      "Epoch 2/80\n",
      "loss: 16511512143200256.000000  [   32/ 4000]\n",
      "loss: 10067914442932224.000000  [ 3232/ 4000]\n",
      "Avg loss: 37100943388442624.000000\n",
      "R2 Score: 0.200991\n",
      "Training Loss: 25516127979407672.0000\n",
      "Validation Loss: 37100943388442624.0000\n",
      "Validation R¬≤ Score: 0.2010\n",
      "New best R¬≤ score: 0.2010\n",
      "\n",
      "Epoch 3/80\n",
      "loss: 5630467512467456.000000  [   32/ 4000]\n",
      "loss: 2239794440568832.000000  [ 3232/ 4000]\n",
      "Avg loss: 34072655103524864.000000\n",
      "R2 Score: 0.266669\n",
      "Training Loss: 18395057621225700.0000\n",
      "Validation Loss: 34072655103524864.0000\n",
      "Validation R¬≤ Score: 0.2667\n",
      "New best R¬≤ score: 0.2667\n",
      "\n",
      "Epoch 4/80\n",
      "loss: 4347396498653184.000000  [   32/ 4000]\n",
      "loss: 5031616464289792.000000  [ 3232/ 4000]\n",
      "Avg loss: 33729486855864320.000000\n",
      "R2 Score: 0.274848\n",
      "Training Loss: 14928788236170952.0000\n",
      "Validation Loss: 33729486855864320.0000\n",
      "Validation R¬≤ Score: 0.2748\n",
      "New best R¬≤ score: 0.2748\n",
      "\n",
      "Epoch 5/80\n",
      "loss: 27398064532619264.000000  [   32/ 4000]\n",
      "loss: 87205247906217984.000000  [ 3232/ 4000]\n",
      "Avg loss: 32274862332444672.000000\n",
      "R2 Score: 0.307196\n",
      "Training Loss: 14337524324494214.0000\n",
      "Validation Loss: 32274862332444672.0000\n",
      "Validation R¬≤ Score: 0.3072\n",
      "New best R¬≤ score: 0.3072\n",
      "\n",
      "Epoch 6/80\n",
      "loss: 8749322975838208.000000  [   32/ 4000]\n",
      "loss: 8701824395640832.000000  [ 3232/ 4000]\n",
      "Avg loss: 32258545575526400.000000\n",
      "R2 Score: 0.307363\n",
      "Training Loss: 11279482526596334.0000\n",
      "Validation Loss: 32258545575526400.0000\n",
      "Validation R¬≤ Score: 0.3074\n",
      "New best R¬≤ score: 0.3074\n",
      "\n",
      "Epoch 7/80\n",
      "loss: 7291847835648000.000000  [   32/ 4000]\n",
      "loss: 11276582664536064.000000  [ 3232/ 4000]\n",
      "Avg loss: 33769389526351872.000000\n",
      "R2 Score: 0.275293\n",
      "Training Loss: 9740321023030460.0000\n",
      "Validation Loss: 33769389526351872.0000\n",
      "Validation R¬≤ Score: 0.2753\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 8/80\n",
      "loss: 868911343992832.000000  [   32/ 4000]\n",
      "loss: 22901199413968896.000000  [ 3232/ 4000]\n",
      "Avg loss: 31906835938148352.000000\n",
      "R2 Score: 0.316867\n",
      "Training Loss: 8586086553632113.0000\n",
      "Validation Loss: 31906835938148352.0000\n",
      "Validation R¬≤ Score: 0.3169\n",
      "New best R¬≤ score: 0.3169\n",
      "\n",
      "Epoch 9/80\n",
      "loss: 9163204278091776.000000  [   32/ 4000]\n",
      "loss: 6495672031248384.000000  [ 3232/ 4000]\n",
      "Avg loss: 30988577735180288.000000\n",
      "R2 Score: 0.333772\n",
      "Training Loss: 9116216582106776.0000\n",
      "Validation Loss: 30988577735180288.0000\n",
      "Validation R¬≤ Score: 0.3338\n",
      "New best R¬≤ score: 0.3338\n",
      "\n",
      "Epoch 10/80\n",
      "loss: 6834529918517248.000000  [   32/ 4000]\n",
      "loss: 6976017415536640.000000  [ 3232/ 4000]\n",
      "Avg loss: 32196855685185536.000000\n",
      "R2 Score: 0.311685\n",
      "Training Loss: 7793536173539852.0000\n",
      "Validation Loss: 32196855685185536.0000\n",
      "Validation R¬≤ Score: 0.3117\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 11/80\n",
      "loss: 492147652427776.000000  [   32/ 4000]\n",
      "loss: 3036061110370304.000000  [ 3232/ 4000]\n",
      "Avg loss: 32280186355449856.000000\n",
      "R2 Score: 0.308634\n",
      "Training Loss: 7327017494252618.0000\n",
      "Validation Loss: 32280186355449856.0000\n",
      "Validation R¬≤ Score: 0.3086\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 12/80\n",
      "loss: 1466714554368000.000000  [   32/ 4000]\n",
      "loss: 6597113790070784.000000  [ 3232/ 4000]\n",
      "Avg loss: 33901722904559616.000000\n",
      "R2 Score: 0.275886\n",
      "Training Loss: 6328587176342716.0000\n",
      "Validation Loss: 33901722904559616.0000\n",
      "Validation R¬≤ Score: 0.2759\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 13/80\n",
      "loss: 4424260575559680.000000  [   32/ 4000]\n",
      "loss: 781356221071360.000000  [ 3232/ 4000]\n",
      "Avg loss: 32437787143700480.000000\n",
      "R2 Score: 0.306548\n",
      "Training Loss: 6359989691885814.0000\n",
      "Validation Loss: 32437787143700480.0000\n",
      "Validation R¬≤ Score: 0.3065\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 14/80\n",
      "loss: 7451587333062656.000000  [   32/ 4000]\n",
      "loss: 515191024386048.000000  [ 3232/ 4000]\n",
      "Avg loss: 30484371156762624.000000\n",
      "R2 Score: 0.345333\n",
      "Training Loss: 5123244989089841.0000\n",
      "Validation Loss: 30484371156762624.0000\n",
      "Validation R¬≤ Score: 0.3453\n",
      "New best R¬≤ score: 0.3453\n",
      "\n",
      "Epoch 15/80\n",
      "loss: 3222790987579392.000000  [   32/ 4000]\n",
      "loss: 4848266457907200.000000  [ 3232/ 4000]\n",
      "Avg loss: 32383621918621696.000000\n",
      "R2 Score: 0.305606\n",
      "Training Loss: 4388489689045139.5000\n",
      "Validation Loss: 32383621918621696.0000\n",
      "Validation R¬≤ Score: 0.3056\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 16/80\n",
      "loss: 915321485524992.000000  [   32/ 4000]\n",
      "loss: 3660513116422144.000000  [ 3232/ 4000]\n",
      "Avg loss: 35207889232592896.000000\n",
      "R2 Score: 0.246558\n",
      "Training Loss: 4964177585203315.0000\n",
      "Validation Loss: 35207889232592896.0000\n",
      "Validation R¬≤ Score: 0.2466\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 17/80\n",
      "loss: 31924987979890688.000000  [   32/ 4000]\n",
      "loss: 2665545556033536.000000  [ 3232/ 4000]\n",
      "Avg loss: 32002167267131392.000000\n",
      "R2 Score: 0.313898\n",
      "Training Loss: 4679502356956578.0000\n",
      "Validation Loss: 32002167267131392.0000\n",
      "Validation R¬≤ Score: 0.3139\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 18/80\n",
      "loss: 453732055646208.000000  [   32/ 4000]\n",
      "loss: 573466851934208.000000  [ 3232/ 4000]\n",
      "Avg loss: 31244781250150400.000000\n",
      "R2 Score: 0.328696\n",
      "Training Loss: 4467423714702524.5000\n",
      "Validation Loss: 31244781250150400.0000\n",
      "Validation R¬≤ Score: 0.3287\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 19/80\n",
      "loss: 6641477044142080.000000  [   32/ 4000]\n",
      "loss: 902937282871296.000000  [ 3232/ 4000]\n",
      "Avg loss: 31420898657435648.000000\n",
      "R2 Score: 0.327166\n",
      "Training Loss: 4356503578580353.0000\n",
      "Validation Loss: 31420898657435648.0000\n",
      "Validation R¬≤ Score: 0.3272\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 20/80\n",
      "loss: 2057874926731264.000000  [   32/ 4000]\n",
      "loss: 3366249337716736.000000  [ 3232/ 4000]\n",
      "Avg loss: 29935782944833536.000000\n",
      "R2 Score: 0.356568\n",
      "Training Loss: 3457176461742440.5000\n",
      "Validation Loss: 29935782944833536.0000\n",
      "Validation R¬≤ Score: 0.3566\n",
      "New best R¬≤ score: 0.3566\n",
      "\n",
      "Epoch 21/80\n",
      "loss: 6227996784459776.000000  [   32/ 4000]\n",
      "loss: 1156095271763968.000000  [ 3232/ 4000]\n",
      "Avg loss: 36768570155204608.000000\n",
      "R2 Score: 0.210783\n",
      "Training Loss: 3876497840440082.5000\n",
      "Validation Loss: 36768570155204608.0000\n",
      "Validation R¬≤ Score: 0.2108\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 22/80\n",
      "loss: 432158132928512.000000  [   32/ 4000]\n",
      "loss: 2843450349191168.000000  [ 3232/ 4000]\n",
      "Avg loss: 32118487363092480.000000\n",
      "R2 Score: 0.311877\n",
      "Training Loss: 3316971527991197.5000\n",
      "Validation Loss: 32118487363092480.0000\n",
      "Validation R¬≤ Score: 0.3119\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 23/80\n",
      "loss: 854759091208192.000000  [   32/ 4000]\n",
      "loss: 423467602149376.000000  [ 3232/ 4000]\n",
      "Avg loss: 30222094969077760.000000\n",
      "R2 Score: 0.351109\n",
      "Training Loss: 3110594796138266.5000\n",
      "Validation Loss: 30222094969077760.0000\n",
      "Validation R¬≤ Score: 0.3511\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 24/80\n",
      "loss: 357367820582912.000000  [   32/ 4000]\n",
      "loss: 1633798848512000.000000  [ 3232/ 4000]\n",
      "Avg loss: 32716221702995968.000000\n",
      "R2 Score: 0.299399\n",
      "Training Loss: 3271192264877015.0000\n",
      "Validation Loss: 32716221702995968.0000\n",
      "Validation R¬≤ Score: 0.2994\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 25/80\n",
      "loss: 637137191960576.000000  [   32/ 4000]\n",
      "loss: 252195899441152.000000  [ 3232/ 4000]\n",
      "Avg loss: 31433080308760576.000000\n",
      "R2 Score: 0.326514\n",
      "Training Loss: 3243605686780690.5000\n",
      "Validation Loss: 31433080308760576.0000\n",
      "Validation R¬≤ Score: 0.3265\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 26/80\n",
      "loss: 939155534118912.000000  [   32/ 4000]\n",
      "loss: 724184837652480.000000  [ 3232/ 4000]\n",
      "Avg loss: 30849266276630528.000000\n",
      "R2 Score: 0.340190\n",
      "Training Loss: 2562347504418947.0000\n",
      "Validation Loss: 30849266276630528.0000\n",
      "Validation R¬≤ Score: 0.3402\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 27/80\n",
      "loss: 752788850081792.000000  [   32/ 4000]\n",
      "loss: 2234062001405952.000000  [ 3232/ 4000]\n",
      "Avg loss: 31065790207754240.000000\n",
      "R2 Score: 0.332581\n",
      "Training Loss: 2939230621441982.5000\n",
      "Validation Loss: 31065790207754240.0000\n",
      "Validation R¬≤ Score: 0.3326\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 28/80\n",
      "loss: 858458937098240.000000  [   32/ 4000]\n",
      "loss: 1126258402394112.000000  [ 3232/ 4000]\n",
      "Avg loss: 32130171955838976.000000\n",
      "R2 Score: 0.310843\n",
      "Training Loss: 3032031970377859.0000\n",
      "Validation Loss: 32130171955838976.0000\n",
      "Validation R¬≤ Score: 0.3108\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 29/80\n",
      "loss: 1585094187810816.000000  [   32/ 4000]\n",
      "loss: 387125803483136.000000  [ 3232/ 4000]\n",
      "Avg loss: 31728155634958336.000000\n",
      "R2 Score: 0.319421\n",
      "Training Loss: 2278071009628127.0000\n",
      "Validation Loss: 31728155634958336.0000\n",
      "Validation R¬≤ Score: 0.3194\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 30/80\n",
      "loss: 561288539275264.000000  [   32/ 4000]\n",
      "loss: 148656938287104.000000  [ 3232/ 4000]\n",
      "Avg loss: 31587398617923584.000000\n",
      "R2 Score: 0.321232\n",
      "Training Loss: 2533490602100654.0000\n",
      "Validation Loss: 31587398617923584.0000\n",
      "Validation R¬≤ Score: 0.3212\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 31/80\n",
      "loss: 1143479744856064.000000  [   32/ 4000]\n",
      "loss: 2413917380804608.000000  [ 3232/ 4000]\n",
      "Avg loss: 32132027381710848.000000\n",
      "R2 Score: 0.310853\n",
      "Training Loss: 2502898877665902.5000\n",
      "Validation Loss: 32132027381710848.0000\n",
      "Validation R¬≤ Score: 0.3109\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 32/80\n",
      "loss: 3320872739799040.000000  [   32/ 4000]\n",
      "loss: 703559733608448.000000  [ 3232/ 4000]\n",
      "Avg loss: 33547163573157888.000000\n",
      "R2 Score: 0.281455\n",
      "Training Loss: 2506151810197094.5000\n",
      "Validation Loss: 33547163573157888.0000\n",
      "Validation R¬≤ Score: 0.2815\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 33/80\n",
      "loss: 612478979407872.000000  [   32/ 4000]\n",
      "loss: 1143549806510080.000000  [ 3232/ 4000]\n",
      "Avg loss: 31907264143032320.000000\n",
      "R2 Score: 0.315640\n",
      "Training Loss: 2157232978864898.0000\n",
      "Validation Loss: 31907264143032320.0000\n",
      "Validation R¬≤ Score: 0.3156\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 34/80\n",
      "loss: 912058115686400.000000  [   32/ 4000]\n",
      "loss: 334609862623232.000000  [ 3232/ 4000]\n",
      "Avg loss: 35786668942295040.000000\n",
      "R2 Score: 0.233332\n",
      "Training Loss: 2227646206572494.7500\n",
      "Validation Loss: 35786668942295040.0000\n",
      "Validation R¬≤ Score: 0.2333\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 35/80\n",
      "loss: 583245083181056.000000  [   32/ 4000]\n",
      "loss: 603046828572672.000000  [ 3232/ 4000]\n",
      "Avg loss: 30754744968413184.000000\n",
      "R2 Score: 0.339452\n",
      "Training Loss: 1936335493935399.0000\n",
      "Validation Loss: 30754744968413184.0000\n",
      "Validation R¬≤ Score: 0.3395\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 36/80\n",
      "loss: 15478546738708480.000000  [   32/ 4000]\n",
      "loss: 1105656719343616.000000  [ 3232/ 4000]\n",
      "Avg loss: 33768151736909824.000000\n",
      "R2 Score: 0.275554\n",
      "Training Loss: 2008714521882919.0000\n",
      "Validation Loss: 33768151736909824.0000\n",
      "Validation R¬≤ Score: 0.2756\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 37/80\n",
      "loss: 1824991733612544.000000  [   32/ 4000]\n",
      "loss: 1186757848596480.000000  [ 3232/ 4000]\n",
      "Avg loss: 37917014110502912.000000\n",
      "R2 Score: 0.188128\n",
      "Training Loss: 2125575848147812.2500\n",
      "Validation Loss: 37917014110502912.0000\n",
      "Validation R¬≤ Score: 0.1881\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 38/80\n",
      "loss: 6533360906141696.000000  [   32/ 4000]\n",
      "loss: 736636417605632.000000  [ 3232/ 4000]\n",
      "Avg loss: 30393801335898112.000000\n",
      "R2 Score: 0.348123\n",
      "Training Loss: 1832637249188003.7500\n",
      "Validation Loss: 30393801335898112.0000\n",
      "Validation R¬≤ Score: 0.3481\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 39/80\n",
      "loss: 1484195071262720.000000  [   32/ 4000]\n",
      "loss: 5791635086508032.000000  [ 3232/ 4000]\n",
      "Avg loss: 34788680476393472.000000\n",
      "R2 Score: 0.255444\n",
      "Training Loss: 1498008139205705.7500\n",
      "Validation Loss: 34788680476393472.0000\n",
      "Validation R¬≤ Score: 0.2554\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 40/80\n",
      "loss: 2523005792026624.000000  [   32/ 4000]\n",
      "loss: 201430241640448.000000  [ 3232/ 4000]\n",
      "Avg loss: 30801980800630784.000000\n",
      "R2 Score: 0.340015\n",
      "Training Loss: 1779931254783737.7500\n",
      "Validation Loss: 30801980800630784.0000\n",
      "Validation R¬≤ Score: 0.3400\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 41/80\n",
      "loss: 1628492819070976.000000  [   32/ 4000]\n",
      "loss: 228370105237504.000000  [ 3232/ 4000]\n",
      "Avg loss: 31777920649265152.000000\n",
      "R2 Score: 0.318834\n",
      "Training Loss: 1585191793543086.0000\n",
      "Validation Loss: 31777920649265152.0000\n",
      "Validation R¬≤ Score: 0.3188\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 42/80\n",
      "loss: 264290795782144.000000  [   32/ 4000]\n",
      "loss: 160211255951360.000000  [ 3232/ 4000]\n",
      "Avg loss: 33370826493394944.000000\n",
      "R2 Score: 0.285428\n",
      "Training Loss: 1738395866554695.7500\n",
      "Validation Loss: 33370826493394944.0000\n",
      "Validation R¬≤ Score: 0.2854\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 43/80\n",
      "loss: 6778953041707008.000000  [   32/ 4000]\n",
      "loss: 391025768005632.000000  [ 3232/ 4000]\n",
      "Avg loss: 32262619301675008.000000\n",
      "R2 Score: 0.307903\n",
      "Training Loss: 1798557259823841.2500\n",
      "Validation Loss: 32262619301675008.0000\n",
      "Validation R¬≤ Score: 0.3079\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 44/80\n",
      "loss: 323087270674432.000000  [   32/ 4000]\n",
      "loss: 1150884536909824.000000  [ 3232/ 4000]\n",
      "Avg loss: 32944474367721472.000000\n",
      "R2 Score: 0.293883\n",
      "Training Loss: 1686713142874210.2500\n",
      "Validation Loss: 32944474367721472.0000\n",
      "Validation R¬≤ Score: 0.2939\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 45/80\n",
      "loss: 124108801048576.000000  [   32/ 4000]\n",
      "loss: 173390077886464.000000  [ 3232/ 4000]\n",
      "Avg loss: 34008362647552000.000000\n",
      "R2 Score: 0.272527\n",
      "Training Loss: 1247783885572407.2500\n",
      "Validation Loss: 34008362647552000.0000\n",
      "Validation R¬≤ Score: 0.2725\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 46/80\n",
      "loss: 1532397757661184.000000  [   32/ 4000]\n",
      "loss: 448156718333952.000000  [ 3232/ 4000]\n",
      "Avg loss: 30660400508829696.000000\n",
      "R2 Score: 0.342335\n",
      "Training Loss: 1243393521130406.0000\n",
      "Validation Loss: 30660400508829696.0000\n",
      "Validation R¬≤ Score: 0.3423\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 47/80\n",
      "loss: 324698218954752.000000  [   32/ 4000]\n",
      "loss: 5175869383376896.000000  [ 3232/ 4000]\n",
      "Avg loss: 31821079383113728.000000\n",
      "R2 Score: 0.317877\n",
      "Training Loss: 1419199502299955.2500\n",
      "Validation Loss: 31821079383113728.0000\n",
      "Validation R¬≤ Score: 0.3179\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 48/80\n",
      "loss: 112367283208192.000000  [   32/ 4000]\n",
      "loss: 1084675535667200.000000  [ 3232/ 4000]\n",
      "Avg loss: 33822859067392000.000000\n",
      "R2 Score: 0.275217\n",
      "Training Loss: 1191753303281631.2500\n",
      "Validation Loss: 33822859067392000.0000\n",
      "Validation R¬≤ Score: 0.2752\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 49/80\n",
      "loss: 1270643928596480.000000  [   32/ 4000]\n",
      "loss: 40114558337024.000000  [ 3232/ 4000]\n",
      "Avg loss: 33330912087244800.000000\n",
      "R2 Score: 0.287987\n",
      "Training Loss: 1070204856691589.1250\n",
      "Validation Loss: 33330912087244800.0000\n",
      "Validation R¬≤ Score: 0.2880\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 50/80\n",
      "loss: 636779300388864.000000  [   32/ 4000]\n",
      "loss: 1039892716978176.000000  [ 3232/ 4000]\n",
      "Avg loss: 32886576480518144.000000\n",
      "R2 Score: 0.296877\n",
      "Training Loss: 882189044329480.2500\n",
      "Validation Loss: 32886576480518144.0000\n",
      "Validation R¬≤ Score: 0.2969\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 51/80\n",
      "loss: 1960622505852928.000000  [   32/ 4000]\n",
      "loss: 354191692267520.000000  [ 3232/ 4000]\n",
      "Avg loss: 32133142278045696.000000\n",
      "R2 Score: 0.311656\n",
      "Training Loss: 1301111445043282.0000\n",
      "Validation Loss: 32133142278045696.0000\n",
      "Validation R¬≤ Score: 0.3117\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 52/80\n",
      "loss: 134623367528448.000000  [   32/ 4000]\n",
      "loss: 410298393559040.000000  [ 3232/ 4000]\n",
      "Avg loss: 33672478706368512.000000\n",
      "R2 Score: 0.279999\n",
      "Training Loss: 1094616275356745.7500\n",
      "Validation Loss: 33672478706368512.0000\n",
      "Validation R¬≤ Score: 0.2800\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 53/80\n",
      "loss: 414864010903552.000000  [   32/ 4000]\n",
      "loss: 170242420506624.000000  [ 3232/ 4000]\n",
      "Avg loss: 30010187540594688.000000\n",
      "R2 Score: 0.358309\n",
      "Training Loss: 1249663143982923.7500\n",
      "Validation Loss: 30010187540594688.0000\n",
      "Validation R¬≤ Score: 0.3583\n",
      "New best R¬≤ score: 0.3583\n",
      "\n",
      "Epoch 54/80\n",
      "loss: 343667109789696.000000  [   32/ 4000]\n",
      "loss: 180555660394496.000000  [ 3232/ 4000]\n",
      "Avg loss: 31127261591633920.000000\n",
      "R2 Score: 0.334746\n",
      "Training Loss: 1506230818358427.7500\n",
      "Validation Loss: 31127261591633920.0000\n",
      "Validation R¬≤ Score: 0.3347\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 55/80\n",
      "loss: 1191200723828736.000000  [   32/ 4000]\n",
      "loss: 877691263778816.000000  [ 3232/ 4000]\n",
      "Avg loss: 31644741296717824.000000\n",
      "R2 Score: 0.322380\n",
      "Training Loss: 1174162061828030.5000\n",
      "Validation Loss: 31644741296717824.0000\n",
      "Validation R¬≤ Score: 0.3224\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 56/80\n",
      "loss: 131230813126656.000000  [   32/ 4000]\n",
      "loss: 139475053182976.000000  [ 3232/ 4000]\n",
      "Avg loss: 33181433451249664.000000\n",
      "R2 Score: 0.289689\n",
      "Training Loss: 1078817378281717.7500\n",
      "Validation Loss: 33181433451249664.0000\n",
      "Validation R¬≤ Score: 0.2897\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 57/80\n",
      "loss: 330296541052928.000000  [   32/ 4000]\n",
      "loss: 191056285007872.000000  [ 3232/ 4000]\n",
      "Avg loss: 33166072785928192.000000\n",
      "R2 Score: 0.289541\n",
      "Training Loss: 1219075112314077.2500\n",
      "Validation Loss: 33166072785928192.0000\n",
      "Validation R¬≤ Score: 0.2895\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 58/80\n",
      "loss: 481783023927296.000000  [   32/ 4000]\n",
      "loss: 118242714583040.000000  [ 3232/ 4000]\n",
      "Avg loss: 30894744259264512.000000\n",
      "R2 Score: 0.337614\n",
      "Training Loss: 840160089380749.2500\n",
      "Validation Loss: 30894744259264512.0000\n",
      "Validation R¬≤ Score: 0.3376\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 59/80\n",
      "loss: 131047454932992.000000  [   32/ 4000]\n",
      "loss: 312289790001152.000000  [ 3232/ 4000]\n",
      "Avg loss: 31820388715462656.000000\n",
      "R2 Score: 0.318459\n",
      "Training Loss: 869384057320898.5000\n",
      "Validation Loss: 31820388715462656.0000\n",
      "Validation R¬≤ Score: 0.3185\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 60/80\n",
      "loss: 1797998132592640.000000  [   32/ 4000]\n",
      "loss: 1559907425845248.000000  [ 3232/ 4000]\n",
      "Avg loss: 31386909544546304.000000\n",
      "R2 Score: 0.328473\n",
      "Training Loss: 826566956378226.7500\n",
      "Validation Loss: 31386909544546304.0000\n",
      "Validation R¬≤ Score: 0.3285\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 61/80\n",
      "loss: 229235121717248.000000  [   32/ 4000]\n",
      "loss: 528150853320704.000000  [ 3232/ 4000]\n",
      "Avg loss: 31592011127586816.000000\n",
      "R2 Score: 0.322858\n",
      "Training Loss: 1104312923464925.1250\n",
      "Validation Loss: 31592011127586816.0000\n",
      "Validation R¬≤ Score: 0.3229\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 62/80\n",
      "loss: 614685854400512.000000  [   32/ 4000]\n",
      "loss: 469341543858176.000000  [ 3232/ 4000]\n",
      "Avg loss: 32881223592312832.000000\n",
      "R2 Score: 0.296176\n",
      "Training Loss: 794625790011506.7500\n",
      "Validation Loss: 32881223592312832.0000\n",
      "Validation R¬≤ Score: 0.2962\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 63/80\n",
      "loss: 10645452252774400.000000  [   32/ 4000]\n",
      "loss: 562993507074048.000000  [ 3232/ 4000]\n",
      "Avg loss: 33465681064230912.000000\n",
      "R2 Score: 0.282362\n",
      "Training Loss: 982342565795725.2500\n",
      "Validation Loss: 33465681064230912.0000\n",
      "Validation R¬≤ Score: 0.2824\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 64/80\n",
      "loss: 122056721039360.000000  [   32/ 4000]\n",
      "loss: 47190357573632.000000  [ 3232/ 4000]\n",
      "Avg loss: 32791749340430336.000000\n",
      "R2 Score: 0.296938\n",
      "Training Loss: 807984631793057.7500\n",
      "Validation Loss: 32791749340430336.0000\n",
      "Validation R¬≤ Score: 0.2969\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 65/80\n",
      "loss: 224810198106112.000000  [   32/ 4000]\n",
      "loss: 409425139466240.000000  [ 3232/ 4000]\n",
      "Avg loss: 32690348182470656.000000\n",
      "R2 Score: 0.299204\n",
      "Training Loss: 877965641221406.7500\n",
      "Validation Loss: 32690348182470656.0000\n",
      "Validation R¬≤ Score: 0.2992\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 66/80\n",
      "loss: 1416835891200000.000000  [   32/ 4000]\n",
      "loss: 113632763445248.000000  [ 3232/ 4000]\n",
      "Avg loss: 33069353393979392.000000\n",
      "R2 Score: 0.293163\n",
      "Training Loss: 779184127260753.8750\n",
      "Validation Loss: 33069353393979392.0000\n",
      "Validation R¬≤ Score: 0.2932\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 67/80\n",
      "loss: 530132242530304.000000  [   32/ 4000]\n",
      "loss: 889315055894528.000000  [ 3232/ 4000]\n",
      "Avg loss: 33580694030516224.000000\n",
      "R2 Score: 0.279826\n",
      "Training Loss: 754482549396340.7500\n",
      "Validation Loss: 33580694030516224.0000\n",
      "Validation R¬≤ Score: 0.2798\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 68/80\n",
      "loss: 332514891661312.000000  [   32/ 4000]\n",
      "loss: 186538566615040.000000  [ 3232/ 4000]\n",
      "Avg loss: 31458562014183424.000000\n",
      "R2 Score: 0.326649\n",
      "Training Loss: 1189354440667168.7500\n",
      "Validation Loss: 31458562014183424.0000\n",
      "Validation R¬≤ Score: 0.3266\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 69/80\n",
      "loss: 47691820171264.000000  [   32/ 4000]\n",
      "loss: 652816976707584.000000  [ 3232/ 4000]\n",
      "Avg loss: 34089190593396736.000000\n",
      "R2 Score: 0.271695\n",
      "Training Loss: 822306353350443.0000\n",
      "Validation Loss: 34089190593396736.0000\n",
      "Validation R¬≤ Score: 0.2717\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 70/80\n",
      "loss: 741663274172416.000000  [   32/ 4000]\n",
      "loss: 296640741113856.000000  [ 3232/ 4000]\n",
      "Avg loss: 32151517574201344.000000\n",
      "R2 Score: 0.311651\n",
      "Training Loss: 768650507769086.0000\n",
      "Validation Loss: 32151517574201344.0000\n",
      "Validation R¬≤ Score: 0.3117\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 71/80\n",
      "loss: 668598230056960.000000  [   32/ 4000]\n",
      "loss: 318637147684864.000000  [ 3232/ 4000]\n",
      "Avg loss: 32595914568761344.000000\n",
      "R2 Score: 0.301549\n",
      "Training Loss: 704243989812871.1250\n",
      "Validation Loss: 32595914568761344.0000\n",
      "Validation R¬≤ Score: 0.3015\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 72/80\n",
      "loss: 873060819271680.000000  [   32/ 4000]\n",
      "loss: 362998354935808.000000  [ 3232/ 4000]\n",
      "Avg loss: 31626576017752064.000000\n",
      "R2 Score: 0.321510\n",
      "Training Loss: 798486328026071.0000\n",
      "Validation Loss: 31626576017752064.0000\n",
      "Validation R¬≤ Score: 0.3215\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 73/80\n",
      "loss: 1064612669685760.000000  [   32/ 4000]\n",
      "loss: 475146460594176.000000  [ 3232/ 4000]\n",
      "Avg loss: 32138168346083328.000000\n",
      "R2 Score: 0.311107\n",
      "Training Loss: 687109669797232.6250\n",
      "Validation Loss: 32138168346083328.0000\n",
      "Validation R¬≤ Score: 0.3111\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 74/80\n",
      "loss: 168348943908864.000000  [   32/ 4000]\n",
      "loss: 328843499929600.000000  [ 3232/ 4000]\n",
      "Avg loss: 32357831344652288.000000\n",
      "R2 Score: 0.306506\n",
      "Training Loss: 714223927102013.5000\n",
      "Validation Loss: 32357831344652288.0000\n",
      "Validation R¬≤ Score: 0.3065\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 75/80\n",
      "loss: 61202319278080.000000  [   32/ 4000]\n",
      "loss: 344110363836416.000000  [ 3232/ 4000]\n",
      "Avg loss: 31633591695835136.000000\n",
      "R2 Score: 0.321686\n",
      "Training Loss: 711357617393369.1250\n",
      "Validation Loss: 31633591695835136.0000\n",
      "Validation R¬≤ Score: 0.3217\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 76/80\n",
      "loss: 402358106324992.000000  [   32/ 4000]\n",
      "loss: 1528447595708416.000000  [ 3232/ 4000]\n",
      "Avg loss: 31429668695441408.000000\n",
      "R2 Score: 0.326202\n",
      "Training Loss: 1163560623477358.5000\n",
      "Validation Loss: 31429668695441408.0000\n",
      "Validation R¬≤ Score: 0.3262\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 77/80\n",
      "loss: 735430941081600.000000  [   32/ 4000]\n",
      "loss: 557500243902464.000000  [ 3232/ 4000]\n",
      "Avg loss: 32012948876558336.000000\n",
      "R2 Score: 0.314483\n",
      "Training Loss: 835504931229139.0000\n",
      "Validation Loss: 32012948876558336.0000\n",
      "Validation R¬≤ Score: 0.3145\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 78/80\n",
      "loss: 227816524218368.000000  [   32/ 4000]\n",
      "loss: 116900889624576.000000  [ 3232/ 4000]\n",
      "Avg loss: 33889990479970304.000000\n",
      "R2 Score: 0.274491\n",
      "Training Loss: 886799026706448.3750\n",
      "Validation Loss: 33889990479970304.0000\n",
      "Validation R¬≤ Score: 0.2745\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 79/80\n",
      "loss: 573601338097664.000000  [   32/ 4000]\n",
      "loss: 89646075740160.000000  [ 3232/ 4000]\n",
      "Avg loss: 32357788613083136.000000\n",
      "R2 Score: 0.307223\n",
      "Training Loss: 615798396321529.8750\n",
      "Validation Loss: 32357788613083136.0000\n",
      "Validation R¬≤ Score: 0.3072\n",
      "No improvement in model learning rate 0.13\n",
      "\n",
      "Epoch 80/80\n",
      "loss: 232229955436544.000000  [   32/ 4000]\n",
      "loss: 2029892946362368.000000  [ 3232/ 4000]\n",
      "Avg loss: 32021502891130880.000000\n",
      "R2 Score: 0.314552\n",
      "Training Loss: 740031139523067.8750\n",
      "Validation Loss: 32021502891130880.0000\n",
      "Validation R¬≤ Score: 0.3146\n",
      "No improvement in model learning rate 0.13\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define hyperparameters at the top\n",
    "lr = 0.13\n",
    "\n",
    "model = MovieMLP(config).to(device)\n",
    "print(model)\n",
    "\n",
    "# Loss function for regression\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=lr,  # Using a smaller learning rate because Lion typically works better with lower values\n",
    "    weight_decay=0.0001,\n",
    "    betas=(0.9, 0.9998)\n",
    ")\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        X = batch['x'].to(device)\n",
    "        lengths = batch['lengths'].to(device)\n",
    "        numeric_features = batch['numeric'].to(device).float()\n",
    "        y = batch['y'].to(device).float()\n",
    "        \n",
    "        pred = model(X, lengths, numeric_features)[:, 0]\n",
    "        loss = loss_fn(pred, y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            current = (batch_idx + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            X = batch['x'].to(device)\n",
    "            lengths = batch['lengths'].to(device)\n",
    "            numeric_features = batch['numeric'].to(device).float()\n",
    "            y = batch['y'].to(device).float()\n",
    "            \n",
    "            pred = model(X, lengths, numeric_features)[:, 0]\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            predictions.extend(pred.cpu().numpy())\n",
    "            actuals.extend(y.cpu().numpy())\n",
    "    \n",
    "    test_loss /= len(dataloader)\n",
    "    r2 = r2_score(actuals, np.array(predictions))\n",
    "    print(f\"Avg loss: {test_loss:>8f}\")\n",
    "    print(f\"R2 Score: {r2:>8f}\") \n",
    "    \n",
    "    return test_loss, r2\n",
    "\n",
    "# Training loop\n",
    "epochs = 80\n",
    "best_r2 = -float('inf')  # Initialize with negative infinity for R¬≤ score\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
    "    val_loss, val_r2 = test(dev_dataloader, model, loss_fn)\n",
    "    \n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Validation R¬≤ Score: {val_r2:.4f}\")\n",
    "    \n",
    "    if val_r2 > best_r2:  # Save model based on R¬≤ score instead of loss\n",
    "        best_r2 = val_r2\n",
    "        print(f\"New best R¬≤ score: {best_r2:.4f}\")\n",
    "        save_path = f\"model_lr_{lr:.4f}.pth\"\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    else:\n",
    "        print(f\"No improvement in model learning rate {lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "61d18125-7e6d-45a7-a5fd-5e4a3b11d15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing final model on test...\n",
      "Avg loss: 33684980685078528.000000\n",
      "R2 Score: 0.211755\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zt/s9ljmn_s4rb2yxy_g0lbzdpc0000gn/T/ipykernel_76438/3565572165.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_path))\n"
     ]
    }
   ],
   "source": [
    "# PART 3e: Load and test the best model\n",
    "\n",
    "# if you want to load the best model from training before testing:\n",
    "model = MovieMLP(config).to(device)\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "print(\"testing final model on test...\")\n",
    "test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf68cd-fb45-4a8c-92ba-23e281ba4f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c99753-20a5-4763-be34-6006ca5506e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
